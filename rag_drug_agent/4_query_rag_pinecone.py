import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_core.documents import Document
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableMap
from pinecone import Pinecone, ServerlessSpec
from langchain_core.output_parsers import StrOutputParser
from langchain_teddynote import logging
# ëª©ì : ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ Pineconeì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ í›„ LLM ì‘ë‹µ ìƒì„±

# LangSmith ì¶”ì  ì„¤ì •
logging.langsmith("4_query_rag_pinecone")

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
PINECONE_INDEX_NAME = os.getenv("PINECONE_INDEX_NAME", "medical-db")
PINECONE_REGION = os.getenv("PINECONE_REGION", "us-east-1")
NAMESPACE = "drug-rag-namespace"

# 2. ëª¨ë¸ ë° ì„ë² ë”© ì´ˆê¸°í™”
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
embedder = OpenAIEmbeddings(model="text-embedding-3-large")

# 3. Pinecone ì¸ë±ìŠ¤ ì¤€ë¹„
def get_or_create_index():
    pc = Pinecone(api_key=PINECONE_API_KEY)

    if PINECONE_INDEX_NAME not in pc.list_indexes().names():
        pc.create_index(
            name=PINECONE_INDEX_NAME,
            dimension=3072,
            metric="dotproduct",
            spec=ServerlessSpec(cloud="aws", region=PINECONE_REGION)
        )
        print(f"âœ… ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ: {PINECONE_INDEX_NAME}")
    return pc.Index(PINECONE_INDEX_NAME)

index = get_or_create_index()

# 4. ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
def similarity_search(query, top_k=5):
    embedded_query = embedder.embed_query(query)
    result = index.query(vector=embedded_query, top_k=top_k, namespace=NAMESPACE, include_metadata=True)
    return [
        Document(page_content=match["metadata"].get("itemName", "") + "\n" + match["metadata"].get("text", ""))
        for match in result["matches"]
    ]

# 5. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = PromptTemplate.from_template("""
ë„ˆëŠ” ì˜ì•½í’ˆ ì •ë³´ë¥¼ ì„¤ëª…í•´ì£¼ëŠ” ì „ë¬¸ê°€ì•¼. ì•„ë˜ì˜ ì•½í’ˆ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì‚¬ìš©ì ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜.

ì•½í’ˆ ì •ë³´:
{context}

ì‚¬ìš©ì ì§ˆë¬¸:
{question}
""")

# 6. ì²´ì¸ êµ¬ì„±
rag_chain = (
    RunnableMap({
        "context": lambda x: "\n\n".join([doc.page_content for doc in similarity_search(x["question"])]),
        "question": lambda x: x["question"]
    })
    | prompt
    | llm
    | StrOutputParser()
)

# 7. CLI ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸ’¬ ì•½í’ˆ ì§ˆë¬¸ ì‹œìŠ¤í…œ (Pinecone + LLM)")
    try:
        while True:
            query = input("ğŸ” ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'exit'): ")
            if query.lower() in ["exit", "quit"]:
                break
            response = rag_chain.invoke({"question": query})
            print(f"ğŸ§  ì‘ë‹µ: {response}\n")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")