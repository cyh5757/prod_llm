import os
from pprint import pprint
from dotenv import load_dotenv
import logging

from langchain_community.document_loaders import JSONLoader
from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import ChatOpenAI, OpenAIEmbeddings

from ragas.testset.generator import TestsetGenerator
from ragas.testset.evolutions import simple, reasoning, multi_context, conditional
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from ragas.testset.extractor import KeyphraseExtractor
from ragas.testset.docstore import InMemoryDocumentStore

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def main():
    try:
        # âœ… í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸° (API KEY ë“±)
        load_dotenv()

        # âœ… ë¬¸ì„œ ë¡œë”©
        loader = JSONLoader(
            file_path="distilated_snack_data/Vectordb_formatted_snack_data.json",
            jq_schema=".[]",         # JSON ë°°ì—´ì˜ ê° í•­ëª©ì„ ê°œë³„ ë¬¸ì„œë¡œ ë¡œë“œ
            text_content=False       # page_content í•„ë“œê°€ ì´ë¯¸ ì¡´ì¬í•¨
        )
        docs = loader.load()
        logger.info(f"âœ… ì´ {len(docs)}ê°œ ë¬¸ì„œ ë¡œë“œë¨")
        logger.debug("ğŸ“„ ì²« ë¬¸ì„œ ë‚´ìš© ì¼ë¶€:\n%s", docs[0].page_content[:300])

        # âœ… ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì •ë¦¬
        for doc in docs:
            doc.metadata["filename"] = doc.metadata.get("source", "unknown")

        # âœ… LLM ë° ì„ë² ë”© ì„¤ì •
        generator_llm = ChatOpenAI(model="gpt-4")  # ëª¨ë¸ëª… ìˆ˜ì •
        critic_llm = ChatOpenAI(model="gpt-4")     # ëª¨ë¸ëª… ìˆ˜ì •
        embeddings = OpenAIEmbeddings(model="text-embedding-3-small")

        # âœ… ë˜í¼ ë° ë„êµ¬ êµ¬ì„±
        langchain_llm = LangchainLLMWrapper(generator_llm)
        ragas_embeddings = LangchainEmbeddingsWrapper(embeddings)
        keyphrase_extractor = KeyphraseExtractor(llm=langchain_llm)
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)

        # âœ… ë¬¸ì„œ ì €ì¥ì†Œ ì´ˆê¸°í™”
        docstore = InMemoryDocumentStore(
            splitter=splitter,
            embeddings=ragas_embeddings,
            extractor=keyphrase_extractor,
        )

        # âœ… í…ŒìŠ¤íŠ¸ì…‹ ìƒì„±ê¸° êµ¬ì„±
        generator = TestsetGenerator.from_langchain(
            generator_llm,
            critic_llm,
            ragas_embeddings,
            docstore=docstore,
        )

        # âœ… ì§ˆë¬¸ ìœ í˜• ë¹„ìœ¨ ì„¤ì •
        distributions = {
            simple: 0.4,
            reasoning: 0.2,
            multi_context: 0.2,
            conditional: 0.2,
        }

        # âœ… í…ŒìŠ¤íŠ¸ì…‹ ìƒì„±
        testset = generator.generate_with_langchain_docs(
            documents=docs[:5],  # ë” ì‘ì€ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸
            test_size=3,         # ë” ì‘ì€ í…ŒìŠ¤íŠ¸ í¬ê¸°
            distributions=distributions,
            with_debugging_logs=True,
            raise_exceptions=False,  # ì˜ˆì™¸ ë°œìƒ ì‹œ ê²½ê³ ë§Œ í‘œì‹œ
        )

        # âœ… ê²°ê³¼ ì €ì¥
        df = testset.to_pandas()
        df.to_csv("distilated_snack_data/ragas_synthetic_dataset.csv", index=False)
        logger.info("âœ… í…ŒìŠ¤íŠ¸ì…‹ CSV ì €ì¥ ì™„ë£Œ")

    except Exception as e:
        logger.error("ì˜¤ë¥˜ ë°œìƒ: %s", str(e), exc_info=True)
        raise

if __name__ == "__main__":
    main()
